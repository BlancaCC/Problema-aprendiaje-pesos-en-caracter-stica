\section{Algoritmo de búsqueda local}  

No es difícil pensar que no todos los atributos tengan la misma relevancia que otros, es por ellos que podría refinarse la distancia usada en $1-$NN por una ponderada como 

\begin{equation}
    d_e(e_1, e_2)
     = 
     \sqrt{
         \sum_i
             w_i \cdot (e^i_1 - e^i_2)^2  
        +
        \sum_j w_j \cdot d_h(e_1^j, e_2^j)
         }
\end{equation}

El vector de pesos $w \in [0,1]^n$ es nuestra incógnita
y deberemos de encontrar el que mejore el clasificador de ponderación 1.

Además puesto que no tenemos variables nominales el resultado será
la distancia final será 

\begin{equation}
  d_e(e_1, e_2)
   = 
   \sqrt{
       \sum_i
           w_i \cdot (e^i_1 - e^i_2)^2  
       }
\end{equation}

\subsection{Componentes del algoritmo de búsqueda local}  
Además el hecho de poder ponderar la relevancia de los atributos abre la puerta a nuevas oportunidades  como la eliminación de ruido o 
plantearse un estudio más formar sobre la explicación o relevancia de ciertos atributos a la hora de observar un fenómeno. 

Las métricas a observar de nuestro algoritmo son: 

\begin{itemize}
    \item \textbf{Precisión} o \texttt{tasa-clasificación} Rendimiento promedio para $k=1$ y utilizado \textit{leave one out}.
    \item \textbf{Tasa de reducción}, número de características que se consideran como clasificador. 
    \begin{equation}
        \text{tasa-reducción} 
        = 
        100
        \frac{\text{número de }w_i < 0.1}{\text{número de características}}.
    \end{equation}
    \item \textbf{Función de evaluación}\label{funcionEvaluacion} Permite cuantificar el éxito de nuestra selección de pesos de acorde a una  combinación de precisión y simplicidad, su expresión viene dada por 
    \begin{equation}
        F(w) = 
            \alpha \texttt{tasa-clasificación}
            +
            (1 - \alpha) \texttt{tasa-reducción}.
    \end{equation}
    Nótese que $\alpha$ es la ponderación de relevancia que se le da al modelo.
\end{itemize}

\subsection{Búsqueda local del primero mejor}  

El algoritmo que vamos a usar es el conocido como el del \textit{primero mejor} y que radica en esencia de que  cuanto se genera 
una solución que mejora a la actual se aplica el movimiento y se pasa a la siguiente iteración.

Se detiene la búsqueda cuando se haya generado un número máximo de vecinos que no mejora el resultado
o un número máximo de generaciones en las que mejora.

Descripción del algoritmo, necesitamos primero tener una función para general vecinos, de acorde a los requisitos, esta será 
la función \texttt{GeneraVecino($w,\sigma$)} que devolverá un vector 


  Para general el vecino hemos usado este algoritmo 

  \begin{algorithm}[H]
    \caption{Genera nuevo vecino}
    \hspace*{\algorithmicindent} 

        \textbf{Entrada}: $w$ vector inicio de dimensión $n$. 
        \hspace*{\algorithmicindent} 
        \textbf{Salida}:
        Vector de pesos $w_{vecino}$.        
    \begin{algorithmic}[1]
      \Procedure{GeneraVecino}{$w$,}   
          \State $z \gets \mathcal{N}(0, \sigma)_{1 \times n}$
          \Comment{vector generado a partir de distribución normal de media 0 y desviación típica $\sigma$}
        \For{$i \in \{1, \ldots, n\}$}
        \Comment{Truncamos entre cero y uno cada componente}
          \State $w_{vecino}[i] \gets \max(0, \min(1, w[i]-z[i]))$
        \EndFor
        \State \textbf{return} $w_{vecino}$
      \EndProcedure
    \end{algorithmic}
  \end{algorithm}

El vector generado de manera aleatoria ha sido implementado con la función \texttt{rand}función en Julia.  
Denotando además a $F$ por la 
función de \textit{fitness} que responde a la ecuación 
(\ref{funcionEvaluacion}), 
donde la \textit{tasa de clasificación} se obtiene por medio del algoritmo de 
\textit{leave one out} presentado en clase de prácticas.

\begin{algorithm}[H]
  \caption{Búsqueda local del primero mejor}
      \hspace*{\algorithmicindent} 
      \textbf{Salida}:
      Vector de pesos $w$.        
  \begin{algorithmic}[1]
    \Procedure{PrimeroMejor}{$numeroMaximoVecinosSinMejora$,
    $evaluacionesMaximasFuncionObjetivo$
    $numeroAtributos$, 
    $umbralAtributo$, 
    $F$}
          \Comment{Inicializamos pesos}
        \State $w \gets$ vector aleatorio  
        \State $iteracionesSinMejora \gets 0$ \Comment{Indica número de generaciones}
      \While{$iteracionesSinMejora  < numeroMaximoVecinosSinMejora$ and  $evaluaciones< evaluacionesMaximasFuncionObjetivo$}
      \State $w_{vecino} \gets$ \texttt{GeneraVecino($w,\sigma$)}
      \If{ $F(w_{vecino}) > F(w)$}
        \State $w \gets w_{vecino}$
        \State $iteracionesSinMejora  \gets 0$ 
      \EndIf
      \State $iteracionesSinMejora  \gets iteraciones +1$
      \State $evaluaciones \gets evaluaciones +1$
      \EndWhile
      \State \textbf{return} $w$
    \EndProcedure
  \end{algorithmic}
\end{algorithm}

Notemos que el orden del algoritmo recae en la complejidad de \textit{leave one out}
que es cuadrático tan cual lo hemos implementado. 
Pero además siendo estrictos las constantes ocultas son bastante elevadas, ya que serían $\mathcal{O}(K n^2)$ con el máximo de iteraciones que será a lo sumo $15000$ y si tenemos en cuenta que el tamaño de la bases de datos tiene la que más 355 ejemplos, nos estamos enfrentando a un algoritmo que se comporta como un cúbico, luego a priori ya podemos predecir que tardará bastante más que el algoritmo 1NN sin modificación,

\subsubsection{Resultados búsqueda local}

\subsubsection*{Ionosphere} 

\begin{table}[H]
  \centering
  \begin{tabular}{|c|c|c|c|c|}
    \hline
    & \multicolumn{4}{|c|}{\textit{Ionosphere}}  \\
    \hline
    &	Clasificación &		Reducción	
    &	Agregación	&	Tiempo en ms \\
    \hline
    Partición 1	& 83,099	 & 29,412	& 56,255 & 159947,335 \\
    Partición 2 &	85,714	 &23,529 &	54,622&	155736,745 \\
    Partición 3 &	80,0 &	29,412&	54,706 & 141024,197 \\
    Partición 4	& 90, & 32,353  & 61,176  & 161423,94 \\
    Partición 5	 &87,143  & 20,588  & 53,866 & 155231,525 \\
    \hline
    Medias 	 & 85,191  & 27,059  & 56,125 & 154672,748 \\
    \hline
    Desviación típica &	3,825 &	4,833& 2,954 &	8079,714 \\ 
    \hline  
  \end{tabular}
  \caption{Resultados búsqueda local para los datos \textbf{ionosphere}}
  \label{table:BL_ionosphere}
\end{table}

Se han analizado a demás los pesos resultantes de cada partición, obteniendo con ello: 
Un vector de pesos medio con un redondeo de tres decimales es 

\begin{align*}
w_{medio} = [ 
  & 0.172; 0.202; 0.451; 0.328; 0.424; 0.655; 0.421; 0.63; 0.766; 0.496; 0.282; 0.477; 0.51;\\
 & 0.408; 0.656; 0.3; 0.73; 0.677; 0.755; 0.447;
  0.703; 0.69; 0.568; 0.604; 0.226; 0.658; 0.472; \\
 & 0.608; 0.344; 0.621; 0.205; 0.328; 0.369; 0.634
 ]
\end{align*}

Donde cada coeficiente del vector presenta una desviación típica media de 

\begin{align*}
  w_{desv. tip.} = [ 
    & 0.208; 0.415; 0.396; 0.398; 0.386; 0.428; 0.372; 0.477; 0.43; 0.415; 0.434; 0.364; 0.494; 0.514 \\
& 0.514; 0.481; 0.353; 0.283; 0.411; 0.344; 0.443; 0.216; 0.411; 0.192; 0.534; 0.282; 0.436; 0.298 \\
& 0.298; 0.38; 0.355; 0.383; 0.168; 0.264; 0.423; 0.44
   ]
  \end{align*}

  La media de de las coeficientes de la desviación típica es de $0.377$, esto es un valor bastante elevado teniendo presente de que el rango de valores 
  que pueden tomar los coeficientes de los pesos es de $[0,1]$ 
  sumado a esto y que ninguna de las desviaciones típicas es
  próxima a $0$, no tenemos ninguna hipótesis de que verdaderamente 
  existan atributos más relevantes que otros para clasificar los datos.


\subsubsection*{Parkinson} 

\begin{table}[H]
  \centering
  \begin{tabular}{|c|c|c|c|c|}
    \hline
    & \multicolumn{4}{|c|}{\textit{Parkirsons}}  \\
    \hline
    &	Clasificación &		Reducción	
    &	Agregación	&	Tiempo en ms \\
    \hline
    Partición 1 & 92.308 &  18.182 &  55.245 &  24649.99  \\
    Partición 2 &	  97.436 &  18.182 &  57.809 &  34156.963  \\
    Partición 3 &	  92.308 &  27.273 &  59.79 &  28148.808   \\
    Partición 4	&   92.308 &  13.636 &  52.972 &  80035.187   \\
    Partición 5	 &   89.744 &  40.909 &  65.326 &  38188.808   \\
    \hline
    Medias 	 &   92.821 &  23.636 &  58.228 &  41035.951   \\
    \hline
    Desviación típica &	  2.809 &  10.852 &  4.731 &  22421.782   \\ 
    \hline  
  \end{tabular}
  \caption{Resultados búsqueda local para los datos \textbf{Parkinsons}}
  \label{table:BL_Parkinson}
\end{table}


El vector de pesos medio es 

\begin{align*}
  w_{medio} = [ 
    & 0.355, 0.527, 0.305, 0.407, 0.546, 0.3, 0.454, 0.675, 0.585, 0.709, 0.478, 0.281, 0.553, 0.427 \\
    & 0.427, 0.451, 0.452, 0.309, 0.747, 0.817, 0.695, 0.644, 0.644]
  \end{align*}
  
  Donde cada coeficiente del vector presenta una desviación típica media de 
  
  \begin{align*}
    w_{desv. tip.} = [ 
      & 0.33, 0.364, 0.341, 0.364, 0.511, 0.19, 0.4, 0.44, 0.374, 0.288, 0.44, 0.334, 0.514, 0.505 \\
      & 0.505, 0.508, 0.512, 0.426, 0.359, 0.207, 0.45, 0.368, 0.448
     ]
\end{align*}

El valor de peso medio es de $0.516$ y la desviación típica de $0.158$, luego por media todos los pesos son por lo general relevantes. 

Además la media de de las coeficientes de la desviación típica es de $0.394$, esto es un valor bastante elevado teniendo presente de que el rango de valores 
que pueden tomar los coeficientes de los pesos es de $[0,1]$ 
sumado a esto y que ninguna de las desviaciones típicas es
próxima a $0$, no tenemos ninguna hipótesis de que verdaderamente 
existan atributos más relevantes que otros para clasificar los datos.


%%% Estudio para heart attack
\subsubsection*{Speft heart} 

\begin{table}[H]
  \centering
  \begin{tabular}{|c|c|c|c|c|}
    \hline
    & \multicolumn{4}{|c|}{\textit{Spectf heart}}  \\
    \hline
    &	Clasificación &		Reducción	
    &	Agregación	&	Tiempo en ms \\
    \hline
  
    Partición 1 &    84.286   &  15.909   &  50.097   &  244436.767   \\
    Partición 2 &	      77.143   &  25.0   &  51.071   &  336259.139  \\
    Partición 3 &	     84.286   &  25.0   &  54.643   &  423946.195   \\
    Partición 4	&     85.714   &  22.727   &  54.221   &  382986.496   \\
    Partición 5	 &      82.609   &  13.636   &  48.123   &  401840.577   \\
    \hline
    Medias 	 &      82.807   &  20.455   &  51.631   &  357893.835   \\
    \hline
    Desviación típica &	    3.352   &  5.33   &  2.773   &  71176.609   \\ 
    \hline  
  \end{tabular}
  \caption{Resultados búsqueda local para los datos \textbf{Spectf heart}}
  \label{table:BL_Spectf-heart}
\end{table}


El vector de pesos medio es 

\begin{align*}
  w_{medio} = [ 
    &  0.446, 0.
    372, 0.574, 0.364, 0.373, 0.502, 0.443, 0.366, 0.545, 0.659, 0.403, 0.698, 0.419, 0.541 \\
    &  0.541, 0.572, 0.802, 0.65, 0.516, 0.348, 0.315, 0.416, 0.718, 0.292, 0.52, 0.346, 0.471, 0.57  \\
    &  0.57, 0.098, 0.574, 0.691, 0.646, 0.413, 0.33, 0.624, 0.187, 0.425, 0.451, 0.839, 0.612, 0.523  \\
    &  0.523, 0.489, 0.676, 0.623, 0.577  
    ]
  \end{align*}
  
  Donde cada coeficiente del vector presenta una desviación típica media de 
  
  \begin{align*}
    w_{desv. tip.} = [ 
      &  0.207, 0.305, 0.373, 0.475, 0.35, 0.346, 0.516, 0.322, 0.308, 0.253, 0.401, 0.31, 0.531, 0.345  \\
      &  0.345, 0.282, 0.192, 0.335, 0.331, 0.39, 0.409, 0.477, 0.308, 0.373, 0.413, 0.21, 0.453, 0.394  \\
      &  0.394, 0.137, 0.408, 0.436, 0.193, 0.342, 0.42, 0.456, 0.134, 0.389, 0.365, 0.256, 0.482, 0.205  \\
      & [0.205, 0.339, 0.463, 0.369, 0.37
     ]
\end{align*}

El valor de peso medio es de $0.500$ y la desviación típica de $0.155$, luego por media todos los pesos son por lo general relevantes. 

Además la media de de las coeficientes de la desviación típica es de $0.349$, esto es un valor bastante elevado teniendo presente de que el rango de valores 
que pueden tomar los coeficientes de los pesos es de $[0,1]$ 
sumado a esto y que ninguna de las desviaciones típicas es
próxima a $0$, no tenemos ninguna hipótesis de que verdaderamente 
existan atributos más relevantes que otros para clasificar los datos.

\subsubsection*{ Otras consideraciones }
Si comparamos los resultados obtenidos en clasificación en comparativa con los del 1NN con la distancia euclidea \ref{Tabla:1nn-base} podemos observar que son peores, esto era esperable ya que la función que estamos optimizando otorga la misma importancia a la tasa de clasificación y a la de reducción.  

Los tiempo han aumentado además considerablemente 

Si utilizamos la métrica 
\begin{equation}\label{metricaNuevaTiempo}
    \frac{\text{tiempo búsqueda local}}{\text{tiempo 1NN}} 
    \times 100
\end{equation}

De media en ionosfera tenemos un incremento de más de un   $3.346 \times 10^6 \%$ en Parkinsons 
de $1.775 \times 10^6 \%$ y finalmente de 
Spectf heart 
$8.701 \times 10^6 \%$, 
a la vista de los resultados, cabe cuestionarse duramente si verdaderamente merece la pena tal coste computacional. 

Sin embargo si este tipo de aproximaciones fuera relevante para algún tipo de problema cabría una posibilidad para reducir el coste computacional y es que el cuello de botella reside en el uso del algoritmo de \textit{leave one out} utilizada en el cálculo del \textit{fitness} en \ref{funcionEvaluacion} utilizando técnicas menos precisas como
  realizar solo \textit{leave one out} para un conjunto menor de datos, por ejemplo la mitad.


Respecto al tiempo de cómputo global, también se podría reducir, como hemos hecho nosotros, paralelizando el algoritmo, pero la bondad de esta implementación recaería en en la arquitectura.

  

  





